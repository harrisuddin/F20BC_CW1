{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CW Doc: https://canvas.hw.ac.uk/courses/20749/files/2193508?wrap=1"
      ],
      "metadata": {
        "id": "hto8PLeVw57d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVvGYIIHtvHX"
      },
      "outputs": [],
      "source": [
        "# setup\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "  \n",
        "  # Initalize the layer\n",
        "  def __init__(self, num_inputs, num_neurons):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.01 * np.random.randn(num_inputs, num_neurons)\n",
        "    self.biases = np.zeros((1, num_neurons))\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate the output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases"
      ],
      "metadata": {
        "id": "TXcyeT6FuJoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid activation function\n",
        "class ActFn_Sigmoid:\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    self.output = 1/(1 + np.exp(-inputs))"
      ],
      "metadata": {
        "id": "w1bub8N6uRX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU activation fuction\n",
        "class ActFn_ReLU:\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.maximum(0, inputs)"
      ],
      "metadata": {
        "id": "TviKMcufxOA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperbolic tangent function\n",
        "class ActFn_Tanh:\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.tanh(inputs)"
      ],
      "metadata": {
        "id": "dYhNVnnj2GPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = ActFn_Tanh()\n",
        "\n",
        "# Make a forward pass of our training data through this layer\n",
        "dense1.forward([12, 99])\n",
        "\n",
        "# Make a forward pass through activation function\n",
        "# it takes the output of first dense layer here\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "print(activation1.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAWeAF8v2c11",
        "outputId": "6436d9ec-0dad-47f2-b7da-296bc0385c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.08610209  0.49351745 -0.11109536]]\n"
          ]
        }
      ]
    }
  ]
}